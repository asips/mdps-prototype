#!/usr/bin/env python3
"""Download contents of STAC FeatureCollection from ASIPS DawgFS API.
"""
import argparse
import hashlib
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse

import requests
from pystac import Asset, Catalog, CatalogType, Item, ItemCollection, layout

if sys.version_info < (3, 10):
    raise RuntimeError(f"Python >= 3.10 is required, got {sys.version}")

LOG = logging

parser = argparse.ArgumentParser(description=__doc__)
parser.add_argument(
    "--token-file",
    type=Path,
    default=Path.home() / ".config/asipscli.json",
    help="API token",
)
parser.add_argument(
    "--output",
    type=Path,
    default="output.json",
    help="JSON docs containing downloaded files",
)
parser.add_argument(
    "--outdir", type=Path, default=Path("outputs"), help="directory to save outputs to"
)
parser.add_argument("input", type=Path, help="STAC Feature Collection doc")
args = parser.parse_args()

logging.basicConfig(level=logging.INFO, format="%(message)s")

doc = json.load(open(args.input))


def get_token() -> str:
    LOG.info("loading token from %s", args.token_file)
    if not args.token_file.exists():
        raise ValueError(f"token file {args.token_file} does not exist")
    return json.load(open(args.token_file)).get("token")


def download(url: str, outdir: Path) -> Path:
    fname = Path(urlparse(url).path).name
    fpath = outdir / fname
    resp = session.get(url, stream=True, timeout=10)
    resp.raise_for_status()
    expected_csum = resp.headers.get("Digest", "").replace("md5=", "")
    csum = hashlib.md5()
    with open(fpath, "wb") as fp:
        for block in resp.iter_content(chunk_size=2**20):
            fp.write(block)
            csum.update(block)
    got_csum = csum.hexdigest()
    if expected_csum:
        if expected_csum != got_csum:
            raise ValueError(f"Expected md5 {expected_csum}, got {got_csum}")
    else:
        LOG.warning("did not receive checksum in Digest header")
    return Path(fpath)


def get_data_asset(item: Item) -> Asset | None:
    """ Get urn for the first asset with "data" role"""
    assets = [a for a in item.assets.values() if "data" in (a.roles or [])]
    return assets[0] if assets else None


if not args.outdir.is_dir():
    args.outdir.mkdir(exist_ok=True, mode=0o775)

args.outdir.mkdir(exist_ok=True, mode=0o755)

collection = ItemCollection.from_file(args.input)

# reuse connections
session = requests.Session()
session.headers = {"x-api-token": get_token()}


def parse_timestamp(v: str) -> datetime:
    return datetime.strptime(v, "%Y-%m-%dT%H:%M:%S.%fZ")


def pformat(x):
    return json.dumps(x.to_dict(), indent=2)


catalog = Catalog(
    id="local",
    description="Results from local download",
    href=str(args.outdir / "catalog.json"),
    catalog_type=CatalogType.SELF_CONTAINED,
)
errs: list[Item] = []
for item in ItemCollection.from_file(args.input):
    asset = get_data_asset(item)
    if asset is None:
        LOG.warning("no data asset found for item, skipping: %s", item)
        continue
    LOG.info("downloading %s", pformat(item))
    try:
        fpath = download(asset.href, args.outdir)
    except (IOError, ValueError, requests.HTTPError) as err:
        LOG.warning("failed %s: %s", err, item)
        errs.append(item)
        continue

    catalog.add_item(
        Item(
            id=fpath.name,
            geometry=None,
            bbox=None,
            properties={},
            href=str(fpath.with_suffix(fpath.suffix + ".json")),
            datetime=parse_timestamp(item.properties["start_datetime"]),
            start_datetime=parse_timestamp(item.properties["start_datetime"]),
            end_datetime=parse_timestamp(item.properties["end_datetime"]),
            assets={"data": Asset(href=fpath.name, roles=asset.roles)},
        ),
        strategy=layout.AsIsLayoutStrategy(),
    )
    LOG.info("finished %s", asset.href)

catalog.save()
