#!/usr/bin/env python3
import argparse
import json
import logging
import subprocess
from datetime import datetime, timezone
from pathlib import Path

import ccsds
import fnmeta
from pystac import Catalog, ItemCollection

LOG = logging.getLogger("level0split")


def run(cmd, **kwds) -> subprocess.CompletedProcess:
    kwds.setdefault("check", True)
    cmd = [str(x) for x in cmd]
    LOG.info("%s", " ".join(cmd))
    return subprocess.run(cmd, **kwds)


def gather_catalog_inputs(fpath: Path, roles: list[str]) -> list[Path]:
    """Gather input file paths from STAC catalog inputs.

    Assumes the asset hrefs are local.
    """
    LOG.info("splitting %s", fpath)
    output_paths = []
    data = json.load(fpath.open())
    if data.get("type", "") == "FeatureCollection":
        avail_items = list(ItemCollection.from_dict(data).items)
    else:
        avail_items = list(Catalog.from_dict(data, href=str(fpath)).get_items(recursive=True))
    LOG.info("%s catalog items", len(avail_items))  # type: ignore
    for item in avail_items:
        LOG.debug(item)
        for asset in item.assets.values():
            if not set(asset.roles or []) & set(roles):
                LOG.debug(
                    "skipping item %s asset role %s href %s",
                    item.id,
                    asset.roles,
                    asset.href,
                )
                continue
            if asset.href.startswith("/"):
                path = Path(asset.href)
            else:
                path = fpath.parent / Path(asset.href)
            if not path.exists():
                LOG.warning(
                    "Item %s asset role %s href %s does not exist locally",
                    item.id,
                    asset.roles,
                    path,
                )
                continue
            if not is_level0_pds(path.name):
                LOG.warning(
                    "skipping file not identifiable by fnmeta as level0 pds: %s", path
                )
                continue
            output_paths.append(path)
    return output_paths


spacecraft_apids = [0, 8, 11, 30, 34, 37]
atms_apids = [515, 528, 530, 531]
cris_apids = [1289, 1290] + list(range(1315, 1396))
viirs_apids = list(range(800, 828))
interesting_apids = set(spacecraft_apids + atms_apids + cris_apids + viirs_apids)


def has_timecode(pkt: ccsds.Packet) -> bool:
    return pkt.header.apid in interesting_apids


def is_level0_pds(name: str) -> bool:
    return "rec_type" in (fnmeta.identify(name) or {})


def split_one(fpath: Path, dest: Path, file_duration: int) -> list[Path]:
    assert "AT" in fpath.name, "Failed level0 pds sanity check"
    outputs = {}
    for group in ccsds.decode_packet_groups(str(fpath)):
        if len(group.packets) == 0 or not group.packets[0].header.has_secondary_header:
            continue
        # packets all have CDS timecodes at start of secondary header
        timecode = (
            ccsds.decode_cds_timecode(bytes(group.packets[0].data[6:15])) / 1000
        )
        bucket = timecode - (timecode % file_duration)
        if bucket not in outputs:
            # first packet for this bucket, create new file in dest with 6t name
            dt = datetime.fromtimestamp(bucket, tz=timezone.utc)
            fname = f"{fpath.name[:20]}6{fpath.name[21]}{dt:%y%j%H%M%S}001.PDS"
            outputs[bucket] = (dest / fname).open("wb")
        for pkt in group.packets:
            outputs[bucket].write(bytes(pkt.data))

    return [o.name for o in outputs.values()]


def main(
    collection_id: str,
    input_: Path,
    catalog_name: str,
    file_duration: int,
    destdir: Path,
    roles: list[str] | None = None,
):
    if input_.is_dir():
        catalog = input_ / catalog_name
        inputs = gather_catalog_inputs(catalog, roles=roles or [])
    else:
        inputs = [input_]

    for fpath in inputs:
        for output in split_one(fpath, destdir, file_duration):
            LOG.info("%s -> %s", fpath, output)

    run(
        [
            "catgen",
            "--verbose",
            collection_id,
            f"{destdir}/*.PDS",
        ],
        check=True,
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--verbose", action="store_true")
    parser.add_argument(
        "-c",
        "--catalog-name",
        # default="catalog.json",
        default="stage-in-results.json",
        help="Name of catalog/collection file in input dir",
    )
    parser.add_argument(
        "-d",
        "--file-duration",
        type=int,
        default=360,
        help="File duration in seconds. Files will be aligned to 0z",
    )
    parser.add_argument(
        "input",
        type=Path,
        help="Path to a directory containing a STAC feature collection JSON file",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        default=Path("."),
        help="directory to save outputs to",
    )
    parser.add_argument("collection_id")
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format="%(name)s -- %(message)s",
    )

    if not args.input.exists():
        parser.error(f"input {args.input} does not exist")

    main(
        args.collection_id,
        args.input,
        args.catalog_name,
        args.file_duration,
        destdir=args.output,
        roles=["data"],
    )
