#!/usr/bin/env python3
"""Split 2h Level-0 Time-based files into 6-min files."""
import argparse
import json
import logging
import subprocess
from datetime import datetime, timedelta, timezone
from pathlib import Path

from pystac import Asset, Catalog, CatalogType, Item, ItemCollection

LOG = logging


def parse_timestamp(fname: str) -> datetime:
    # P1590011AAAAAAAAAAAAAT19245030010901.PDS
    s = fname.split("T")[-1]
    return datetime.strptime(s[:11], "%y%j%H%M%S")


def run(cmd, **kwds) -> subprocess.CompletedProcess:
    kwds.setdefault("check", True)
    cmd = [str(x) for x in cmd]
    LOG.info("%s", " ".join(cmd))
    return subprocess.run(cmd, **kwds)


def get_dest(fpath: Path, dt: datetime, dest: Path) -> Path:
    return dest / f"{fpath.name[:20]}6T{dt:%y%j%H%M%S}{fpath.name[-7:]}"


def get_times(fpath: Path) -> tuple[datetime, datetime]:
    """Use the first packet time in the file to determine the start/end times of the file."""
    cmd = [
        "ccsds",
        "info",
        "--format=json",
        fpath,
    ]
    proc = run(cmd, capture_output=True)
    zult = json.loads(proc.stdout)
    dt = datetime.strptime(
        zult["summary"]["first_packet_time"],
        "%Y-%m-%dT%H:%M:%S.%fZ",
    ).replace(tzinfo=timezone.utc)

    return (
        start := datetime.fromtimestamp(
            dt.timestamp() - dt.timestamp() % 7200, timezone.utc
        ),
        start + timedelta(hours=2),
    )


def split_one(fpath: Path, dest: Path) -> list[Path]:
    outputs = []
    start, _end = get_times(fpath)
    LOG.info("splitting file %s for time %s", fpath.name, start)
    for i in range(20):
        gran_start = start + timedelta(minutes=i * 6)
        gran_end = gran_start + timedelta(minutes=6)
        destpath = get_dest(fpath, gran_start, dest)
        cmd = [
            "ccsds",
            "filter",
            "--clobber",
            "-o",
            str(destpath),
            "--before",
            (gran_start - timedelta(seconds=10)).strftime("%Y-%m-%dT%H:%M:%SZ"),
            "--after",
            (gran_end + timedelta(seconds=10)).strftime("%Y-%m-%dT%H:%M:%SZ"),
            str(fpath),
        ]
        run(cmd)
        outputs.append(destpath)

    return outputs


def gather_inputs(fpath: Path, roles: list[str]) -> list[Path]:
    """Gather input file paths from STAC catalog inputs.

    Assumes the asset hrefs are local.
    """
    output_paths = []
    data = json.load(fpath.open())
    if data.get("type", "") == "FeatureCollection":
        avail_items = ItemCollection.from_dict(data).items
    else:
        avail_items = Catalog.from_dict(data).get_items(recursive=True)
    LOG.info("%s catalog items", len(avail_items))  # type: ignore
    for item in avail_items:
        LOG.debug(item)
        for asset in item.assets.values():
            if not set(asset.roles or []) & set(roles):
                LOG.debug(
                    "skipping item %s asset role %s href %s",
                    item.id,
                    asset.roles,
                    asset.href,
                )
                continue
            if asset.href.startswith("/"):
                path = Path(asset.href)
            else:
                path = fpath.parent / Path(asset.href).name
            if not path.exists():
                LOG.warning(
                    "Item %s asset role %s href %s does not exist locally",
                    item.id,
                    asset.roles,
                    path,
                )
                continue
            output_paths.append(path)
    return output_paths


def write_catalog(outputs: list[Path], destdir: Path):
    """Write a STAC FeatureCollection for our putputs."""
    catalog = Catalog("level0split", "level0split outputs")
    for fpath in outputs:
        fname = fpath.name
        dt = parse_timestamp(fname)
        item = Item(
            id=fpath.name.rsplit(".", 1)[0],
            geometry=None,
            bbox=None,
            properties={},
            datetime=dt,
            start_datetime=dt,
            end_datetime=dt + timedelta(minutes=6),
            assets={"data": Asset(href=str(fpath.absolute()), roles=["data"])},
        )
        catalog.add_item(item)
    catalog.normalize_hrefs(str(destdir))
    catalog.make_all_asset_hrefs_relative()
    catalog.save(catalog_type=CatalogType.SELF_CONTAINED)


def main(indir: Path, catalog_name: str, roles: list[str], destdir: Path):
    outputs = []
    catalog = indir / catalog_name
    for fpath in gather_inputs(catalog, roles=roles):
        LOG.info("splitting %s", fpath)
        outputs += split_one(fpath, destdir)

    write_catalog(outputs, destdir)
    LOG.info("Wrote catalog to %s", destdir)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--verbose", action="store_true")
    parser.add_argument(
        "-c",
        "--catalog-name",
        # default="catalog.json",
        default="stage-in-results.json",
        help="Name of catalog/collection file in input dir",
    )
    parser.add_argument(
        "input",
        type=Path,
        help="Path to STAC feature collection containing all relavent inputs",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        default=Path("."),
        help="directory to save outputs to",
    )
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO, format="%(message)s"
    )

    if not args.input.exists():
        parser.error(f"input {args.input} does not exist")

    main(args.input, args.catalog_name, roles=["data"], destdir=args.output)
