#!/usr/bin/env python3
"""Download contents of STAC FeatureCollection from ASIPS DawgFS API.
"""
import argparse
import hashlib
import json
import logging
import os
from pathlib import Path
from urllib.parse import urlparse

import requests

LOG = logging

parser = argparse.ArgumentParser(description=__doc__)
parser.add_argument("--token", help="API token")
parser.add_argument("--output", type=Path, default="output.json", help="JSON docs containing downloaded files")
parser.add_argument("input", type=Path, help="STAC doc json")
args = parser.parse_args()

logging.basicConfig(level=logging.INFO, format="%(message)s")

doc = json.load(open(args.doc))


def get_token() -> str:
    if args.token is not None:
        return args.token
    path = Path(os.environ.get("HOME") or "") / ".config/asipscli.json"
    if not path.exists():
        raise ValueError(f"token not provided and {path} does not exist")
    return json.load(open(path)).get("token")


def download(url: str) -> Path:
    fname = Path(urlparse(url).path).name
    resp = session.get(url, stream=True, timeout=10)
    resp.raise_for_status()
    expected_csum = resp.headers.get("Digest", "").replace("md5=", "")
    csum = hashlib.md5()
    with open(fname, "wb") as fp:
        for block in resp.iter_content(chunk_size=2**20):
            fp.write(block)
            csum.update(block)
    got_csum = csum.hexdigest()
    if expected_csum:
        if expected_csum != got_csum:
            raise ValueError(f"Expected md5 {expected_csum}, got {got_csum}")
    else:
        LOG.warning("did not receive checksum in Digest header")
    return Path(fname)


def geturl(item: dict) -> str | None:
    assets = [a["href"] for a in item.get("assets", []) if a["type"] == "data"]
    return assets[0] if assets else None


# reuse connections
session = requests.Session()
session.headers = {"x-api-token": get_token()}

output = {"files": []}
for item in doc.get("items", []):
    url = geturl(item)
    if url is None:
        LOG.warning("no url found for item, skipping: %s", item)
        continue
    LOG.info("downloading %s", url)
    fpath = download(url)
    output["files"].append(str(fpath))

json.dump(output, open(args.output, "wt"), indent=2)
    LOG.info("finished %s", fpath)
